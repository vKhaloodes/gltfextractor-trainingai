import os
import json
import glob
import random
import numpy as np
from typing import List


import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# =========================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø© (Ø¹Ø¯Ù‘Ù„ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ù‡Ù†Ø§)
# =========================
DATA_DIR   = "data"  # Ù…Ø¬Ù„Ø¯ JSON Ø§Ù„Ù…ÙˆØ­Ù‘Ø¯
OUT_DIR    = "model"
SAMPLES_DIR = os.path.join(OUT_DIR, "samples")
os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(SAMPLES_DIR, exist_ok=True)

SEED = 42
BATCH_SIZE = 8
EPOCHS = 200
Z_DIM = 128                 # Ø·ÙˆÙ„ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡
LR_G = 1e-4                 # ØªØ¹Ù„Ù… Ø§Ù„Ù…ÙˆÙ„Ù‘Ø¯
LR_D = 1e-4                 # ØªØ¹Ù„Ù… Ø§Ù„Ù…Ù…ÙŠÙ‘Ø²
BETA1, BETA2 = 0.0, 0.9     # ÙˆÙÙ‚ WGAN-GP paper
N_CRITIC = 5                # Ø¹Ø¯Ø¯ Ø®Ø·ÙˆØ§Øª D Ù„ÙƒÙ„ Ø®Ø·ÙˆØ© G
LAMBDA_GP = 10.0
NUM_WORKERS = 0             # Ø¹Ù„Ù‰ ÙˆÙŠÙ†Ø¯ÙˆØ² Ø®Ù„Ù‡ 0
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
PRINT_EVERY = 1
SAVE_EVERY  = 10            # Ø­ÙØ¸ Ø¹ÙŠÙ†Ø§Øª ÙƒÙ„ X Ø¥ÙŠØ¨ÙÙˆÙƒ

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# =========================
# Dataset
# =========================
class UnifiedJSONPointDataset(Dataset):
    """
    ÙŠØªÙˆÙ‚Ø¹ Ù…Ù„ÙØ§Øª JSON ÙÙŠÙ‡Ø§: "vertices": [[x,y,z], ...]
    ÙˆÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ù†ÙØ³ Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø· (N,3).
    """
    def __init__(self, folder: str):
        self.files = sorted(glob.glob(os.path.join(folder, "*.json")))
        if len(self.files) == 0:
            raise ValueError(f"No JSON files found in {folder}")
        with open(self.files[0], "r", encoding="utf-8") as f:
            d0 = json.load(f)
        self.N = len(d0.get("vertices", []))
        if self.N == 0:
            raise ValueError("First JSON has zero vertices or missing 'vertices' key")

        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙˆØ­ÙŠØ¯
        for p in self.files:
            with open(p, "r", encoding="utf-8") as f:
                d = json.load(f)
            n = len(d.get("vertices", []))
            if n != self.N:
                raise ValueError(f"Inconsistent vertex count: {p} has {n}, expected {self.N}")

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        p = self.files[idx]
        with open(p, "r", encoding="utf-8") as f:
            d = json.load(f)
        verts = np.asarray(d.get("vertices", []), dtype=np.float32)  # (N,3), Ù†Ø·Ø§Ù‚Ù‡Ø§ ÙŠÙÙØªØ±Ø¶ [-1,1]
        return torch.from_numpy(verts)  # (N,3)


# =========================
# Ù†Ù…Ø§Ø°Ø¬ WGAN-GP (Point Cloud)
# =========================
class Generator(nn.Module):
    """
    ÙŠÙØ­ÙˆÙ‘Ù„ z âˆˆ R^{Z_DIM} Ø¥Ù„Ù‰ Ø³Ø­Ø§Ø¨Ø© Ù†Ù‚Ø§Ø· (N,3) Ø¯Ø§Ø®Ù„ [-1,1] Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… tanh.
    Ø¨Ø³ÙŠØ· ÙˆÙØ¹Ø§Ù„ Ù„Ù„Ø¨Ø¯Ø¡Ø› ØªÙ‚Ø¯Ø± ØªØ·ÙˆØ±Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§ (MLP Ø£ÙƒØ¨Ø± / Conditioning / FoldingNet...).
    """
    def __init__(self, z_dim=128, num_points=2048):
        super().__init__()
        self.num_points = num_points
        self.net = nn.Sequential(
            nn.Linear(z_dim, 1024), nn.ReLU(True),
            nn.Linear(1024, 2048), nn.ReLU(True),
            nn.Linear(2048, num_points * 3),
            nn.Tanh()  # Ù…Ù„Ø§Ø¦Ù… Ù„Ø£Ù† Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ù…ÙØ·Ø¨Ù‘Ø¹Ø© Ø¥Ù„Ù‰ [-1,1]
        )

    def forward(self, z):              # z: (B, Z_DIM)
        x = self.net(z)                # (B, N*3)
        return x.reshape(-1, self.num_points, 3)  # (B, N, 3)


class Discriminator(nn.Module):
    """
    Ù…Ù…ÙŠÙ‘Ø² Ø¨Ù†Ù…Ø· PointNet ØµØºÙŠØ±: Conv1d 1x1 + Global MaxPool + MLP
    ÙŠØ®Ø±Ø¬ Ù‚ÙŠÙ…Ø© Ø³ÙƒØ± (realness score) Ø¨Ø¯ÙˆÙ† Sigmoid (WGAN loss).
    """
    def __init__(self, num_points=2048):
        super().__init__()
        self.conv1 = nn.Conv1d(3, 64, 1)
        self.conv2 = nn.Conv1d(64, 128, 1)
        self.conv3 = nn.Conv1d(128, 256, 1)
        self.relu  = nn.LeakyReLU(0.2, inplace=True)

        self.fc = nn.Sequential(
            nn.Linear(256, 128), nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(128, 1)  # Ù„Ø§ Sigmoid ÙÙŠ WGAN
        )

    def forward(self, x):  # x: (B, N, 3)
        x = x.transpose(1, 2)        # (B,3,N)
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.relu(self.conv3(x)) # (B,256,N)
        x = torch.max(x, 2)[0]       # (B,256)
        return self.fc(x)            # (B,1)


# =========================
# Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø­ÙØ¸
# =========================
def gradient_penalty(D, real, fake):
    """WGAN-GP gradient penalty"""
    B = real.size(0)
    eps = torch.rand(B, 1, 1, device=real.device)
    interp = eps * real + (1 - eps) * fake
    interp.requires_grad_(True)
    d_interp = D(interp)
    grads = torch.autograd.grad(
        outputs=d_interp,
        inputs=interp,
        grad_outputs=torch.ones_like(d_interp),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]  # (B, N, 3)
    grads = grads.reshape(B, -1)
    gp = ((grads.norm(2, dim=1) - 1.0) ** 2).mean()
    return gp


def save_pointcloud_json(points: np.ndarray, out_path: str):
    """
    ÙŠØ­ÙØ¸ Ù†Ù‚Ø§Ø· (N,3) ÙƒÙ€ JSON Ø¨Ø³ÙŠØ· Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙˆÙ„ÙŠØ¯.
    """
    obj = {"vertices": points.tolist()}
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2, ensure_ascii=False)


def save_pointcloud_png(points: np.ndarray, out_path: str, title="generated"):
    """
    ÙŠØ­ÙØ¸ Ø±Ø³Ù… 3D Ø¨Ø³ÙŠØ· Ù„Ù„Ù†Ù‚Ø§Ø· ÙƒØµÙˆØ±Ø© PNG (Ø§Ø®ØªÙŠØ§Ø±ÙŠ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©).
    """
    fig = plt.figure(figsize=(5,5))
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(points[:,0], points[:,1], points[:,2], s=1)
    ax.set_title(title)
    ax.set_xlabel("X"); ax.set_ylabel("Y"); ax.set_zlabel("Z")
    # Ø§Ø¶Ø¨Ø· Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø­Ø§ÙˆØ± Ù„ØªÙƒÙˆÙ† [-1,1] Ù„Ùˆ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ù…Ø·Ø¨Ø¹Ø©
    ax.set_xlim([-1,1]); ax.set_ylim([-1,1]); ax.set_zlim([-1,1])
    plt.tight_layout()
    plt.savefig(out_path, dpi=150)
    plt.close(fig)

def sample_and_save(G, epoch, num_samples, num_points):
    """
    ÙŠÙˆÙ„Ù‘Ø¯ Ø¹ÙŠÙ†Ø§Øª ÙˆÙŠØ­ÙØ¸Ù‡Ø§ JSON + PNG
    """
    G.eval()
    with torch.no_grad():
        z = torch.randn(num_samples, Z_DIM, device=DEVICE)
        fake = G(z).cpu().numpy()  # (B, N, 3)
    for i in range(num_samples):
        pts = fake[i]
        json_path = os.path.join(SAMPLES_DIR, f"epoch_{epoch:04d}_sample_{i}.json")
        png_path  = os.path.join(SAMPLES_DIR, f"epoch_{epoch:04d}_sample_{i}.png")
        save_pointcloud_json(pts, json_path)


# =========================
# Ø§Ù„ØªØ¯Ø±ÙŠØ¨
# =========================
def main():
    print(f"Device: {DEVICE}")
    dataset = UnifiedJSONPointDataset(DATA_DIR)
    N = dataset.N
    print(f"Found {len(dataset)} samples; unified points per mesh = {N}")

    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,
                        num_workers=NUM_WORKERS, pin_memory=True, drop_last=True)

    G = Generator(z_dim=Z_DIM, num_points=N).to(DEVICE)
    D = Discriminator(num_points=N).to(DEVICE)

    opt_G = torch.optim.Adam(G.parameters(), lr=LR_G, betas=(BETA1, BETA2))
    opt_D = torch.optim.Adam(D.parameters(), lr=LR_D, betas=(BETA1, BETA2))

    step = 0
    best_D = None

    for epoch in range(1, EPOCHS + 1):
        for real in loader:
            real = real.to(DEVICE)  # (B,N,3)

            # -------------------------
            # 1) Train Discriminator
            # -------------------------
            for _ in range(N_CRITIC):
                z = torch.randn(real.size(0), Z_DIM, device=DEVICE)
                fake = G(z).detach()

                d_real = D(real)              # (B,1)
                d_fake = D(fake)              # (B,1)
                gp = gradient_penalty(D, real, fake) * LAMBDA_GP
                d_loss = -(d_real.mean() - d_fake.mean()) + gp

                opt_D.zero_grad(set_to_none=True)
                d_loss.backward()
                opt_D.step()

            # -------------------------
            # 2) Train Generator
            # -------------------------
            z = torch.randn(real.size(0), Z_DIM, device=DEVICE)
            fake = G(z)
            g_loss = -D(fake).mean()

            opt_G.zero_grad(set_to_none=True)
            g_loss.backward()
            opt_G.step()

            step += 1

        if epoch % PRINT_EVERY == 0:
            print(f"[Epoch {epoch:03d}/{EPOCHS}] "
                  f"D_loss: {d_loss.item():.4f}  G_loss: {g_loss.item():.4f}")

        # Ø­ÙØ¸ Ø¹ÙŠÙ†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ
        if epoch % SAVE_EVERY == 0 or epoch == 1 or epoch == EPOCHS:
            sample_and_save(G, epoch, num_samples=3, num_points=N)

        # Ø­ÙØ¸ Ù†Ù‚Ø§Ø· ØªÙØªÙŠØ´ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª
        if epoch % SAVE_EVERY == 0 or epoch == EPOCHS:
            torch.save(G.state_dict(), os.path.join(OUT_DIR, f"G_epoch_{epoch}.pth"))
            torch.save(D.state_dict(), os.path.join(OUT_DIR, f"D_epoch_{epoch}.pth"))

    print("âœ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§ÙƒØªÙ…Ù„.")
    # ØªÙˆÙ„ÙŠØ¯ 10 Ø¹ÙŠÙ†Ø§Øª Ø®ØªØ§Ù…ÙŠØ©
    sample_and_save(G, EPOCHS, num_samples=10, num_points=N)
    print(f"ğŸ“¦ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø¹ÙŠÙ†Ø§Øª ÙÙŠ: {OUT_DIR}")

if __name__ == "__main__":
    main()

#testing for generating vertices of the mesh

import matplotlib.pyplot as plt

# ğŸ”¹ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©: ØªÙˆÙ„ÙŠØ¯ Ø³Ø­Ø§Ø¨Ø§Øª Ù†Ù‚Ø§Ø· Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨
print("\nØ¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ù†Ù…Ø§Ø°Ø¬ Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆÙ„Ù‘Ø¯ Ø§Ù„Ù…Ø¯Ø±Ø¨ ...")

MODEL_PATH = os.path.join("model", "G_epoch_200.pth")
dir_extract = "extracted_data"
GENERATED_DIR = os.path.join(dir_extract, "generated_samples")
os.makedirs(GENERATED_DIR, exist_ok=True)

NUM_SAMPLES = 5      # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ø³Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªÙˆÙ„ÙŠØ¯Ù‡Ø§
NUM_POINTS = 129    # ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ·Ø§Ø¨Ù‚ Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙˆØ­Ø¯

try:
    G = Generator(z_dim=Z_DIM, num_points=NUM_POINTS).to(DEVICE)

    # ØªØ­Ù…ÙŠÙ„ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    state_dict = torch.load(MODEL_PATH, map_location=DEVICE)
    G.load_state_dict(state_dict)
    G.eval()

    print(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…ÙˆÙ„Ù‘Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ù…Ù†: {MODEL_PATH}")

    with torch.no_grad():
        z = torch.randn(NUM_SAMPLES, Z_DIM, device=DEVICE)
        fake_clouds = G(z).cpu().numpy()  # (B, N, 3)

    for i in range(NUM_SAMPLES):
        pts = fake_clouds[i]
        json_path = os.path.join(GENERATED_DIR, f"sample_{i}.json")
        img_path = os.path.join(GENERATED_DIR, f"sample_{i}.png")

        save_pointcloud_json(pts, json_path)
        save_pointcloud_png(pts, img_path, title=f"Generated Sample {i}")

        print(f"ØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {json_path}")

    print("Ø§Ù†ØªÙ‡Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨Ù†Ø¬Ø§Ø­. Ø±Ø§Ø¬Ø¹ Ù…Ø¬Ù„Ø¯ generated_samples.")
except Exception as e:
    print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {e}")
